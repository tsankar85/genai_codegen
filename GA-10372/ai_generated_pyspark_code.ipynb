{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# COMMAND ----------\n#\n",
                "import logging\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, concat, to_date, datediff, sum, avg, expr\nfrom pyspark.sql.types import DoubleType, StringType\n\n# COMMAND ----------\n#\n",
                "# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n#\n",
                "def load_data():\n    \"\"\"Load data from Unity Catalog tables.\"\"\"\n    logger.info(\"Loading data from Unity Catalog tables...\")\n    orders_central_df = spark.table(\"catalog.orders_central\")\n    orders_west_df = spark.table(\"catalog.orders_west\")\n    orders_east_df = spark.table(\"catalog.orders_east\")\n    orders_south_df = spark.table(\"catalog.orders_south_2015\")\n    quota_df = spark.table(\"catalog.quota\")\n    returns_df = spark.table(\"catalog.returns\")\n    return orders_central_df, orders_west_df, orders_east_df, orders_south_df, quota_df, returns_df\n\n# COMMAND ----------\n#\n",
                "def standardize_data(orders_central_df):\n    \"\"\"Standardize data formats and column names.\"\"\"\n    logger.info(\"Standardizing data...\")\n    orders_central_df = orders_central_df.withColumn(\n        \"Order Date\", to_date(concat(\"Order Day\", \"Order Month\", \"Order Year\"), \"ddMMyyyy\")\n    ).withColumn(\n        \"Ship Date\", to_date(concat(\"Ship Day\", \"Ship Month\", \"Ship Year\"), \"ddMMyyyy\")\n    ).withColumnRenamed(\"Discounts\", \"Discount\").withColumnRenamed(\"Product\", \"Product Name\")\n    return orders_central_df\n\n# COMMAND ----------\n#\n",
                "def clean_data(orders_central_df):\n    \"\"\"Clean data by removing nulls and ensuring correct data types.\"\"\"\n    logger.info(\"Cleaning data...\")\n    orders_central_df = orders_central_df.filter(col(\"Order ID\").isNotNull())\n    orders_central_df = orders_central_df.withColumn(\"Sales\", col(\"Sales\").cast(DoubleType())).withColumn(\"Discount\", col(\"Discount\").cast(StringType()))\n    return orders_central_df\n\n# COMMAND ----------\n#\n",
                "def pivot_and_consolidate_data(quota_df, orders_central_df, orders_west_df, orders_east_df, orders_south_df):\n    \"\"\"Pivot quota data and consolidate order data.\"\"\"\n    logger.info(\"Pivoting and consolidating data...\")\n    quota_df = quota_df.select(\n        col(\"Region\"),\n        expr(\"stack(4, '2015', `2015`, '2016', `2016`, '2017', `2017`, '2018', `2018`)\").alias(\"Year\", \"Quota\")\n    )\n    orders_df = orders_central_df.union(orders_west_df).union(orders_east_df).union(orders_south_df)\n    return quota_df, orders_df\n\n# COMMAND ----------\n#\n",
                "def perform_custom_calculations(orders_df, returns_df):\n    \"\"\"Perform custom calculations on the orders data.\"\"\"\n    logger.info(\"Performing custom calculations...\")\n    orders_df = orders_df.withColumn(\"Days to Ship\", datediff(col(\"Ship Date\"), col(\"Order Date\")))\n    is_returned = col(\"Return Reason\").isNotNull()\n    orders_df = orders_df.join(returns_df, \"Order ID\", \"left\").withColumn(\"Returned?\", is_returned)\n    return orders_df\n\n# COMMAND ----------\n#\n",
                "def generate_outputs(orders_df):\n    \"\"\"Generate output data for analysis.\"\"\"\n    logger.info(\"Generating outputs...\")\n    annual_performance_df = orders_df.groupBy(\"Region\", \"Year of Sale\").agg(\n        sum(\"Profit\").alias(\"Total Profit\"),\n        sum(\"Sales\").alias(\"Total Sales\"),\n        sum(\"Quantity\").alias(\"Total Quantity\"),\n        avg(\"Discount\").alias(\"Average Discount\")\n    )\n    return annual_performance_df\n\n# COMMAND ----------\n#\n",
                "def save_outputs(annual_performance_df):\n    \"\"\"Save output data to Unity Catalog tables.\"\"\"\n    logger.info(\"Saving outputs to Unity Catalog tables...\")\n    spark.sql(\"DROP TABLE IF EXISTS catalog.annual_regional_performance\")\n    annual_performance_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.annual_regional_performance\")\n\n# COMMAND ----------\n#\n",
                "def main():\n    try:\n        orders_central_df, orders_west_df, orders_east_df, orders_south_df, quota_df, returns_df = load_data()\n        orders_central_df = standardize_data(orders_central_df)\n        orders_central_df = clean_data(orders_central_df)\n        quota_df, orders_df = pivot_and_consolidate_data(quota_df, orders_central_df, orders_west_df, orders_east_df, orders_south_df)\n        orders_df = perform_custom_calculations(orders_df, returns_df)\n        annual_performance_df = generate_outputs(orders_df)\n        save_outputs(annual_performance_df)\n        logger.info(\"ETL process completed successfully.\")\n    except Exception as e:\n        logger.error(\"An error occurred during the ETL process\", exc_info=True)\n\n# COMMAND ----------\n#\n",
                "# Execute the main function\nmain()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}